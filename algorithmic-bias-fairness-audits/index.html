<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Solid State</title>
        <link href="assets/css/bootstrap.min.css" rel="stylesheet">
        <link rel="stylesheet" href="assets/css/ionicons.min.css">
        <link href="https://fonts.googleapis.com/css?family=Istok+Web:400,400i,700,700i" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=PT+Sans" rel="stylesheet">
        <link href="assets/css/main.css" rel="stylesheet">

    </head>
<body>
    <!-- preloader -->
    <div id="preloader"></div>
    <!-- end of preloader -->

    <div class="body-content" style="display:none;">


        <section id="banner">
            <div class="container">
                <div class="row">
                    <div class="col-md-12">
                        <h2>Algorithmic Bias & Fairness Audits</h2>
                    </div>


                <div class="row">
                    <div class="col-md-12">
                        <p>Ethical Framework: Rawls' Theory of Justice</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="one">
            <div class="container">
                <div class="one-features">
                    <div class="row">
                        <div class="col-md-8">
                            <div class="section-highlight">
                                <h2>OUR MISSION STATEMENT</h2>
                            </div>

                        </div>
                    </div>

                    <div class="row">
                        <div class="col-md-12">
                            <p>Our purpose is to educate and raise awareness of what algorithmic bias is, and how it's consequences affect groups of people.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="two">
            <div class="container">
                <div class="two-features">
                    <div class="row">
                        <div class="col-md-4">
                        <img src="assets/images/algorithm-512.png" alt="Responsive image" class="img-responsive">
                        </div>
                        <div class="col-md-8">
                            <div class="section-highlight">
                                <h2>Algorithmic Bias & Fairness Audits</h2>
                            </div>
                            <div class="section-details">
                                <p>
                                    Algorithmic bias refers to the unfair outcomes that AI systems create.
                                    These outcomes usually reinforce stigmas of racial, gender, and socioeconomic biases.
                                    AI may use biased data with its biased algorithm design, lack of diversity in AI teams, or flawed training data.
                                    Meaning the data that the algorithms are used will have pre-existing prejudices that are baked into the data itself.
                                    That same data feeds back to the AI system, continuing to discriminate against groups of people.

                                </p>
                                <p>
                                    Fairness Audits are type of evaluations that will help asses whether or not the results of any model are properly fair for a variety of different groups.
                                    With these fairness audits, it can help find and mitigate biases before they are used in AI systems.
                                    These audits can implement strategies to promote equal outcomes and understand how AI systems impact different groups.
                                    Organizations can also ensure that AI systems are transparent and accountable by prioritizing fairness audits.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="three">
            <div class="container">
                <div class="three-features">
                    <div class="row">

                        <div class="col-md-8">
                            <div class="section-highlight">
                                <h2>Rawls’ Theory of Justice</h2>
                            </div>
                            <div class="section-details">
                                <p>
                                    An ethical framework that applies to fairness audits is Rawls’ Theory of Justice, which indicates the principles of fairness and equality.
                                </p>
                                <p>
                                    To satisfy a widespread fairness and equality we believe based on the framework that the way AI algorithms should be designed to benefit the least advantaged groups of society.
                                    Algorithms are human made, coming with prenoted bias and beliefs potentially affecting any future algorithms intended purpose.
                                    Individuals should choose principles of justice behind a "veil of ignorance" to be unaware of their attributes, such as race, gender, and social status, to form principles of justice.
                                    And we believe with these fairness audits people can create a society that benefits everyone and prevents biased decision-making.

                                </p>
                            </div>
                        </div>
                        <div class="col-md-4">
                            <img src="assets/images/biasimage2.png" alt="Responsive image" class="img-responsive">
                        </div>

                    </div>
                </div>
            </div>
        </section>

        <section id="five">
            <div class="container">
                <div class="two-features">
                    <div class="row">
                        <div class="col-md-4">
                        <img src="assets/images/biasimage3.png" alt="Responsive image" class="img-responsive">
                        </div>
                        <div class="col-md-8">
                            <div class="section-highlight">
                                <h2>Algorithmic bias is the U.S. healthcare system</h2>
                            </div>
                            <div class="section-details">
                                <p>
                                    <a href="https://www.science.org/doi/10.1126/science.aax2342"
                                            target="_blank">AI systems</a>
                                    revolving around the healthcare system have proven time and again that they fall short of any inclusion efforts.
                                    They are noted to disproportionately affect racial minorities, especially black people, in negative ways.
                                </p>
                                <p>
                                    One well-known example is Optum's Health Risk Prediction Algorithm. It's systems intended purpose was to successfully identify it's patients who were in need of extra care.
                                    Though ultimately it did not fully provide accurate results. It was discovered that the algorithm had underestimated the needs of it's patients who were Black.
                                    The outcome of this led to Black patients ending up with less access and were assumed to be healthier, hence needing less referrals for treatment.
                                </p>
                                <p>
                                    The issue arises when the people providing data and working our healthcare systems impact a bias upon the data.
                                    As it was shown in a 2016 <a href="https://pubmed.ncbi.nlm.nih.gov/27044069/"
                                            target="_blank">study</a>, found that around half of medical students believed that there were biological differences in pain perception between Black and White people.
                                    And as a result we were able to see how a racist untrue myth had negatively impacted an algorithm's design which created a bias that denied the proper care required from Black patients within their system.
                                    Having biases in a environment in the health field such as this could have gravely impacted people's lives, to some even the possibility of dire consequences.
                                    With fairness audits these beliefs would have been retconned and the proper data should have been provided to fix this bias within the algorithms model so the proper care could be provided.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="seven">
            <div class="container">
                <div class="two-features">
                    <div class="row">
                        <div class="col-md-4">
                        <img src="assets/images/realCar.png" alt="Responsive image" class="img-responsive">
                        </div>
                        <div class="col-md-8">
                            <div class="section-highlight">
                                <h2>Algorithmic Bias in AI-Powered Vehicles</h2>
                            </div>
                            <div class="section-details">
                                <p>
                                    Al systems revolving around
                                    <a href="https://www.vox.com/future-perfect/2019/3/5/18251924/self-driving-car-racial-bias-study-autonomous-vehicle-dark-skin"
                                            target="_blank">  AI powered vehicles</a> have shown that they can be bias in complex situtations
                                    especially when protecting pedestrians with diverse skin tones. It's bad because in life or death situations it
                                    will prioritize certain people over others based on race.
                                </p>
                                <p>
                                    One example that backs these claims is a study called Predictive Inequity in Object Detection by the Georgia Institute of Technology.
                                    The study found that object detection models the ones used in self-driving cars were less accurate at identifying pedestrians
                                    with darker skin tones (Fitzpatrick types 4 - 6) detecting them with up to 10% less precision compared to lighter-skinned individuals.
                                    This disparity persists even after controlling for variables like time and day occlusion.
                                </p>
                                <p>
                                    The root of this bias lies in the training datasets. The models had fewer examples of dark-skinned pedestrians, and the loss
                                    functions used during training emphasized accuracy on the more common light skinned examples. Even when they tried to balance this by
                                    increasing the model's attention to dark-skinned pedestrians, the bias still persisted.
                                </p>
                            </div>

                        </div>

                    </div>
                </div>
            </div>
        </section>

        <section id="six">
            <div class="container">
                <div class="three-features">
                    <div class="row">
                        <div class="col-md-9 col-md-offset-1">

		    				<div class="elements-blockquote">

		    					<div class="col-md-12">
		    						<h1 class="elements-h">Practical takeaways</h1>
		    					</div>

		    					<div class="col-md-12">
		    						<p>
                                        To encourage the mitigations of algorithmic biases, organizations should incorporate:
		    						</p>
		    					</div>

		    					<div class="col-md-12">
		    						<ul>
		    							<li>
                                            Regular fairness audits
                                        </li>
		    							<li>
                                            Implement clean diverse datasets to train AI models based on representative data
                                        </li>
		    							<li>
                                            Transparency, providing documents to be held accountable of any results
                                        </li>
		    						</ul>
		    					</div>
		    				</div>

                            <div class="elements-blockquote">

		    					<div class="col-md-12">
		    						<h1 class="elements-h">Links</h1>
		    					</div>
		    					<div class="col-md-12">
		    						<ul>
		    							<li>
                                            <a href="https://www.ibm.com/think/topics/algorithmic-bias" target="_blank">What is algorithmic bias?</a>
                                        </li>
		    							<li>
                                            <a href="https://www.weforum.org/stories/2021/07/ai-machine-learning-bias-discrimination/" target="_blank">Research shows AI is often biased. Here's how to make algorithms work for all of us</a>
                                        </li>
                                        <li>
                                            <a href="https://hbr.org/2018/10/auditing-algorithms-for-bias" target="_blank">Auditing Algorithms for Bias</a>
                                        </li>
                                        <li>
                                            <a href="https://www.vox.com/future-perfect/2019/3/5/18251924/self-driving-car-racial-bias-study-autonomous-vehicle-dark-skin?utm_source=chatgpt.com"
                                            target="_blank">Algorithmic bias in AI-Powered Vehicles</a>
                                        </li>
		    						</ul>
		    					</div>
		    				</div>


                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="footer">
            <div class="container">
                <div class="footer-top">
                    <div class="section-heading">
                        <div class="title">
                            <div class="row">
                                <div class="col-md-12">
                                <h2>Author’s Note</h2>
                                </div>
                            </div>
                        </div>
                        <div class="subtitle">
                            <div class="row">
                                <div class="col-md-9 col-md-offset-1">
                                    <p>
                                        Our group collaborated in discussing and researching how algorithmic bias creates problems to disadvantage groups.
                                        AI models should be implemented for the greater good of everyone and not for a particular group.
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

            </div>
        </section>
    </div>

    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/bootstrap.min.js"></script>
    <script src="assets/js/main.js"></script>


</body>

</html>